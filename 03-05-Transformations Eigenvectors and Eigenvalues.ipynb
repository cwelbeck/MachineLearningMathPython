{
  "cells": [
    {
      "metadata": {
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Transformations, Eigenvectors, and Eigenvalues\n\nMatrices and vectors are used together to manipulate spatial dimensions. This has a lot of applications, including the mathematical generation of 3D computer graphics, geometric modeling, and the training and optimization of machine learning algorithms. We're not going to cover the subject exhaustively here; but we'll focus on a few key concepts that are useful to know when you plan to work with machine learning.\n\n## Linear Transformations\nYou can manipulate a vector by multiplying it with a matrix. The matrix acts a function that operates on an input vector to produce a vector output. Specifically, matrix multiplications of vectors are *linear transformations* that transform the input vector into the output vector.\n\nFor example, consider this matrix ***A*** and vector ***v***:\n\n$$ A = \\begin{bmatrix}2 & 3\\\\5 & 2\\end{bmatrix} \\;\\;\\;\\; \\vec{v} = \\begin{bmatrix}1\\\\2\\end{bmatrix}$$\n\nWe can define a transformation ***T*** like this:\n\n$$ T(\\vec{v}) = A\\vec{v} $$\n\nTo perform this transformation, we simply calculate the dot product by applying the *RC* rule; multiplying each row of the matrix by the single column of the vector:\n\n$$\\begin{bmatrix}2 & 3\\\\5 & 2\\end{bmatrix} \\cdot  \\begin{bmatrix}1\\\\2\\end{bmatrix} = \\begin{bmatrix}8\\\\9\\end{bmatrix}$$\n\nHere's the calculation in Python:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\n\nv = np.array([1,2])\nA = np.array([[2,3],\n              [5,2]])\n\nt = A@v\nprint (t)",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[8 9]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In this case, both the input vector and the output vector have 2 components - in other words, the transformation takes a 2-dimensional vector and produces a new 2-dimensional vector; which we can indicate like this:\n\n$$ T: \\rm I\\!R^{2} \\to \\rm I\\!R^{2} $$\n\nNote that the output vector may have a different number of dimensions from the input vector; so the matrix function might transform the vector from one space to another - or in notation, ${\\rm I\\!R}$<sup>n</sup> -> ${\\rm I\\!R}$<sup>m</sup>.\n\nFor example, let's redefine matrix ***A***, while retaining our original definition of vector ***v***:\n\n$$ A = \\begin{bmatrix}2 & 3\\\\5 & 2\\\\1 & 1\\end{bmatrix} \\;\\;\\;\\; \\vec{v} = \\begin{bmatrix}1\\\\2\\end{bmatrix}$$\n\nNow if we once again define ***T*** like this:\n\n$$ T(\\vec{v}) = A\\vec{v} $$\n\nWe apply the transformation like this:\n\n$$\\begin{bmatrix}2 & 3\\\\5 & 2\\\\1 & 1\\end{bmatrix} \\cdot  \\begin{bmatrix}1\\\\2\\end{bmatrix} = \\begin{bmatrix}8\\\\9\\\\3\\end{bmatrix}$$\n\nSo now, our transformation transforms the vector from 2-dimensional space to 3-dimensional space:\n\n$$ T: \\rm I\\!R^{2} \\to \\rm I\\!R^{3} $$\n\nHere it is in Python:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nv = np.array([1,2])\nA = np.array([[2,3],\n              [5,2],\n              [1,1]])\n\nt = A@v\nprint (t)",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[8 9 3]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nv = np.array([1,2])\nA = np.array([[1,2],\n              [2,1]])\n\nt = A@v\nprint (t)",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[5 4]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Transformations of Magnitude and Amplitude\n\nWhen you multiply a vector by a matrix, you transform it in at least one of the following two ways:\n* Scale the length (*magnitude*) of the matrix to make it longer or shorter\n* Change the direction (*amplitude*) of the matrix\n\nFor example consider the following matrix and vector:\n\n$$ A = \\begin{bmatrix}2 & 0\\\\0 & 2\\end{bmatrix} \\;\\;\\;\\; \\vec{v} = \\begin{bmatrix}1\\\\0\\end{bmatrix}$$\n\nAs before, we transform the vector ***v*** by multiplying it with the matrix ***A***:\n\n\\begin{equation}\\begin{bmatrix}2 & 0\\\\0 & 2\\end{bmatrix} \\cdot  \\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}2\\\\0\\end{bmatrix}\\end{equation}\n\nIn this case, the resulting vector has changed in length (*magnitude*), but has not changed its direction (*amplitude*).\n\nLet's visualize that in Python:"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nv = np.array([1,0])\nA = np.array([[2,0],\n              [0,2]])\n\nt = A@v\nprint (t)\n\n# Plot v and t\nvecs = np.array([t,v])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['blue', 'orange'], scale=10)\nplt.show()",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_501/lib/python3.6/site-packages/matplotlib/font_manager.py:229: UserWarning: Matplotlib is building the font cache using fc-list. This may take a moment.\n  'Matplotlib is building the font cache using fc-list. '\n",
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": "[2 0]\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The original vector ***v*** is shown in orange, and the transformed vector ***t*** is shown in blue - note that ***t*** has the same direction (*amplitude*) as ***v*** but a greater length (*magnitude*).\n\nNow let's use a different matrix to transform the vector ***v***:\n\\begin{equation}\\begin{bmatrix}0 & -1\\\\1 & 0\\end{bmatrix} \\cdot  \\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}0\\\\1\\end{bmatrix}\\end{equation}\n\nThis time, the resulting vector has been changed to a different amplitude, but has the same magnitude."
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nv = np.array([1,0])\nA = np.array([[0,-1],\n              [1,0]])\n\nt = A@v\nprint (t)\n\n# Plot v and t\nvecs = np.array([v,t])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['orange', 'blue'], scale=10)\nplt.show()",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "[0 1]\n",
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Now let's see change the matrix one more time:\n\\begin{equation}\\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix} \\cdot  \\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}2\\\\1\\end{bmatrix}\\end{equation}\n\nNow our resulting vector has been transformed to a new amplitude *and* magnitude - the transformation has affected both direction and scale."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nv = np.array([1,0])\nA = np.array([[2,1],\n              [1,2]])\n\nt = A@v\nprint (t)\n\n# Plot v and t\nvecs = np.array([v,t])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['orange', 'blue'], scale=10)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Afine Transformations\nAn Afine transformation multiplies a vector by a matrix and adds an offset vector, sometimes referred to as *bias*; like this:\n\n$$T(\\vec{v}) = A\\vec{v} + \\vec{b}$$\n\nFor example:\n\n\\begin{equation}\\begin{bmatrix}5 & 2\\\\3 & 1\\end{bmatrix} \\cdot  \\begin{bmatrix}1\\\\1\\end{bmatrix} + \\begin{bmatrix}-2\\\\-6\\end{bmatrix} = \\begin{bmatrix}5\\\\-2\\end{bmatrix}\\end{equation}\n\nThis kind of transformation is actually the basis of linear regression, which is a core foundation for machine learning. The matrix defines the *features*, the first vector is the *coefficients*, and the bias vector is the *intercept*.\n\nhere's an example of an Afine transformation in Python:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nv = np.array([1,1])\nA = np.array([[5,2],\n              [3,1]])\nb = np.array([-2,-6])\n\nt = A@v + b\nprint (t)\n\n# Plot v and t\nvecs = np.array([v,t])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['orange', 'blue'], scale=15)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Eigenvectors and Eigenvalues\nSo we can see that when you transform a vector using a matrix, we change its direction, length, or both. When the transformation only affects scale (in other words, the output vector has a different magnitude but the same amplitude as the input vector), the matrix multiplication for the transformation is the equivalent operation as some scalar multiplication of the vector.\n\nFor example, earlier we examined the following transformation that dot-mulitplies a vector by a matrix:\n\n$$\\begin{bmatrix}2 & 0\\\\0 & 2\\end{bmatrix} \\cdot  \\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}2\\\\0\\end{bmatrix}$$\n\nYou can achieve the same result by mulitplying the vector by the scalar value ***2***:\n\n$$2 \\times \\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}2\\\\0\\end{bmatrix}$$\n\nThe following python performs both of these calculation and shows the results, which are identical."
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nv = np.array([1,0])\nA = np.array([[2,0],\n              [0,2]])\n\nt1 = A@v\nprint (t1)\nt2 = 2*v\nprint (t2)\n\nfig = plt.figure()\na=fig.add_subplot(1,1,1)\n# Plot v and t1\nvecs = np.array([t1,v])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['blue', 'orange'], scale=10)\nplt.show()\na=fig.add_subplot(1,2,1)\n# Plot v and t2\nvecs = np.array([t2,v])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['blue', 'orange'], scale=10)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "In cases like these, where a matrix transformation is the equivelent of a scalar-vector multiplication, the scalar-vector pairs that correspond to the matrix are known respectively as eigenvalues and eigenvectors. We generally indicate eigenvalues using the Greek letter lambda (&lambda;), and the formula that defines eigenvalues and eigenvectors with respect to a transformation is:\n\n$$ T(\\vec{v}) = \\lambda\\vec{v}$$\n\nWhere the vector ***v*** is an eigenvector and the value ***&lambda;*** is an eigenvalue for transformation ***T***.\n\nWhen the transformation ***T*** is represented as a matrix multiplication, as in this case where the transformation is represented by matrix ***A***:\n\n$$ T(\\vec{v}) = A\\vec{v} = \\lambda\\vec{v}$$\n\nThen  ***v*** is an eigenvector and ***&lambda;*** is an eigenvalue of ***A***.\n\nA matrix can have multiple eigenvector-eigenvalue pairs, and you can calculate them manually. However, it's generally easier to use a tool or programming language. For example, in Python you can use the ***linalg.eig*** function, which returns an array of eigenvalues and a matrix of the corresponding eigenvectors for the specified matrix.\n\nHere's an example that returns the eigenvalue and eigenvector pairs for the following matrix:\n\n$$A=\\begin{bmatrix}2 & 0\\\\0 & 3\\end{bmatrix}$$"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nA = np.array([[2,0],\n              [0,3]])\neVals, eVecs = np.linalg.eig(A)\nprint(eVals)\nprint(eVecs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So there are two eigenvalue-eigenvector pairs for this matrix, as shown here:\n\n$$ \\lambda_{1} = 2, \\vec{v_{1}} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}  \\;\\;\\;\\;\\;\\; \\lambda_{2} = 3, \\vec{v_{2}} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} $$\n\nLet's verify that multiplying each eigenvalue-eigenvector pair corresponds to the dot-product of the eigenvector and the matrix. Here's the first pair:\n\n$$ 2 \\times \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix}  \\;\\;\\;and\\;\\;\\; \\begin{bmatrix}2 & 0\\\\0 & 3\\end{bmatrix} \\cdot \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix} $$\n\nSo far so good. Now let's check the second pair:\n\n$$ 3 \\times \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 3\\end{bmatrix}  \\;\\;\\;and\\;\\;\\; \\begin{bmatrix}2 & 0\\\\0 & 3\\end{bmatrix} \\cdot \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 3\\end{bmatrix} $$\n\nSo our eigenvalue-eigenvector scalar multiplications do indeed correspond to our matrix-eigenvector dot-product transformations.\n\nHere's the equivalent code in Python, using the ***eVals*** and ***eVecs*** variables you generated in the previous code cell:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "vec1 = eVecs[:,0]\nlam1 = eVals[0]\n\nprint('Matrix A:')\nprint(A)\nprint('-------')\n\nprint('lam1: ' + str(lam1))\nprint ('v1: ' + str(vec1))\nprint ('Av1: ' + str(A@vec1))\nprint ('lam1 x v1: ' + str(lam1*vec1))\n\nprint('-------')\n\nvec2 = eVecs[:,1]\nlam2 = eVals[1]\n\nprint('lam2: ' + str(lam2))\nprint ('v2: ' + str(vec2))\nprint ('Av2: ' + str(A@vec2))\nprint ('lam2 x v2: ' + str(lam2*vec2))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "You can use the following code to visualize these transformations:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "t1 = lam1*vec1\nprint (t1)\nt2 = lam2*vec2\nprint (t2)\n\nfig = plt.figure()\na=fig.add_subplot(1,1,1)\n# Plot v and t1\nvecs = np.array([t1,vec1])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['blue', 'orange'], scale=10)\nplt.show()\na=fig.add_subplot(1,2,1)\n# Plot v and t2\nvecs = np.array([t2,vec2])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['blue', 'orange'], scale=10)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Similarly, earlier we examined the following matrix transformation:\n\n$$\\begin{bmatrix}2 & 0\\\\0 & 2\\end{bmatrix} \\cdot  \\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}2\\\\0\\end{bmatrix}$$\n\nAnd we saw that you can achieve the same result by mulitplying the vector by the scalar value ***2***:\n\n$$2 \\times \\begin{bmatrix}1\\\\0\\end{bmatrix} = \\begin{bmatrix}2\\\\0\\end{bmatrix}$$\n\nThis works because the scalar value 2 and the vector (1,0) are an eigenvalue-eigenvector pair for this matrix.\n\nLet's use Python to determine the eigenvalue-eigenvector pairs for this matrix:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nA = np.array([[2,0],\n              [0,2]])\neVals, eVecs = np.linalg.eig(A)\nprint(eVals)\nprint(eVecs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So once again, there are two eigenvalue-eigenvector pairs for this matrix, as shown here:\n\n$$ \\lambda_{1} = 2, \\vec{v_{1}} = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}  \\;\\;\\;\\;\\;\\; \\lambda_{2} = 2, \\vec{v_{2}} = \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} $$\n\nLet's verify that multiplying each eigenvalue-eigenvector pair corresponds to the dot-product of the eigenvector and the matrix. Here's the first pair:\n\n$$ 2 \\times \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix}  \\;\\;\\;and\\;\\;\\; \\begin{bmatrix}2 & 0\\\\0 & 2\\end{bmatrix} \\cdot \\begin{bmatrix}1 \\\\ 0\\end{bmatrix} = \\begin{bmatrix}2 \\\\ 0\\end{bmatrix} $$\n\nWell, we already knew that. Now let's check the second pair:\n\n$$ 2 \\times \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 2\\end{bmatrix}  \\;\\;\\;and\\;\\;\\; \\begin{bmatrix}2 & 0\\\\0 & 2\\end{bmatrix} \\cdot \\begin{bmatrix}0 \\\\ 1\\end{bmatrix} = \\begin{bmatrix}0 \\\\ 2\\end{bmatrix} $$\n\nNow let's use Pythonto verify and plot these transformations:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "vec1 = eVecs[:,0]\nlam1 = eVals[0]\n\nprint('Matrix A:')\nprint(A)\nprint('-------')\n\nprint('lam1: ' + str(lam1))\nprint ('v1: ' + str(vec1))\nprint ('Av1: ' + str(A@vec1))\nprint ('lam1 x v1: ' + str(lam1*vec1))\n\nprint('-------')\n\nvec2 = eVecs[:,1]\nlam2 = eVals[1]\n\nprint('lam2: ' + str(lam2))\nprint ('v2: ' + str(vec2))\nprint ('Av2: ' + str(A@vec2))\nprint ('lam2 x v2: ' + str(lam2*vec2))\n\n\n# Plot the resulting vectors\nt1 = lam1*vec1\nt2 = lam2*vec2\n\nfig = plt.figure()\na=fig.add_subplot(1,1,1)\n# Plot v and t1\nvecs = np.array([t1,vec1])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['blue', 'orange'], scale=10)\nplt.show()\na=fig.add_subplot(1,2,1)\n# Plot v and t2\nvecs = np.array([t2,vec2])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['blue', 'orange'], scale=10)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's take a look at one more, slightly more complex example. Here's our matrix:\n\n$$\\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix}$$\n\nLet's get the eigenvalue and eigenvector pairs:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\n\nA = np.array([[2,1],\n              [1,2]])\n\neVals, eVecs = np.linalg.eig(A)\nprint(eVals)\nprint(eVecs)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "This time the eigenvalue-eigenvector pairs are:\n\n$$ \\lambda_{1} = 3, \\vec{v_{1}} = \\begin{bmatrix}0.70710678 \\\\ 0.70710678\\end{bmatrix}  \\;\\;\\;\\;\\;\\; \\lambda_{2} = 1, \\vec{v_{2}} = \\begin{bmatrix}-0.70710678 \\\\ 0.70710678\\end{bmatrix} $$\n\nSo let's check the first pair:\n\n$$ 3 \\times \\begin{bmatrix}0.70710678 \\\\ 0.70710678\\end{bmatrix} = \\begin{bmatrix}2.12132034 \\\\ 2.12132034\\end{bmatrix}  \\;\\;\\;and\\;\\;\\; \\begin{bmatrix}2 & 1\\\\0 & 2\\end{bmatrix} \\cdot \\begin{bmatrix}0.70710678 \\\\ 0.70710678\\end{bmatrix} = \\begin{bmatrix}2.12132034 \\\\ 2.12132034\\end{bmatrix} $$\n\nNow let's check the second pair:\n\n$$ 1 \\times \\begin{bmatrix}-0.70710678 \\\\ 0.70710678\\end{bmatrix} = \\begin{bmatrix}-0.70710678\\\\0.70710678\\end{bmatrix}  \\;\\;\\;and\\;\\;\\; \\begin{bmatrix}2 & 1\\\\1 & 2\\end{bmatrix} \\cdot \\begin{bmatrix}-0.70710678 \\\\ 0.70710678\\end{bmatrix} = \\begin{bmatrix}-0.70710678\\\\0.70710678\\end{bmatrix} $$\n\nWith more complex examples like this, it's generally easier to do it with Python:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "vec1 = eVecs[:,0]\nlam1 = eVals[0]\n\nprint('Matrix A:')\nprint(A)\nprint('-------')\n\nprint('lam1: ' + str(lam1))\nprint ('v1: ' + str(vec1))\nprint ('Av1: ' + str(A@vec1))\nprint ('lam1 x v1: ' + str(lam1*vec1))\n\nprint('-------')\n\nvec2 = eVecs[:,1]\nlam2 = eVals[1]\n\nprint('lam2: ' + str(lam2))\nprint ('v2: ' + str(vec2))\nprint ('Av2: ' + str(A@vec2))\nprint ('lam2 x v2: ' + str(lam2*vec2))\n\n\n# Plot the results\nt1 = lam1*vec1\nt2 = lam2*vec2\n\nfig = plt.figure()\na=fig.add_subplot(1,1,1)\n# Plot v and t1\nvecs = np.array([t1,vec1])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['blue', 'orange'], scale=10)\nplt.show()\na=fig.add_subplot(1,2,1)\n# Plot v and t2\nvecs = np.array([t2,vec2])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['blue', 'orange'], scale=10)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Eigendecomposition\nSo we've learned a little about eigenvalues and eigenvectors; but you may be wondering what use they are. Well, one use for them is to help decompose transformation matrices.\n\nRecall that previously we found that a matrix transformation of a vector changes its magnitude, amplitude, or both. Without getting too technical about it, we need to remember that vectors can exist in any spatial orientation, or *basis*; and the same transformation can be applied in different *bases*.\n\nWe can decompose a matrix using the following formula:\n\n$$A = Q \\Lambda Q^{-1}$$\n\nWhere ***A*** is a trasformation that can be applied to a vector in its current base, ***Q*** is a matrix of eigenvectors that defines a change of basis, and ***&Lambda;*** is a matrix with eigenvalues on the diagonal that defines the same linear transformation as ***A*** in the base defined by ***Q***.\n\nLet's look at these in some more detail. Consider this matrix:\n\n$$A=\\begin{bmatrix}3 & 2\\\\1 & 0\\end{bmatrix}$$\n\n***Q*** is a matrix in which each column is an eigenvector of ***A***; which as we've seen previously, we can calculate using Python:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\n\nA = np.array([[3,2],\n              [1,0]])\n\nl, Q = np.linalg.eig(A)\nprint(Q)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So for matrix ***A***, ***Q*** is the following matrix:\n\n$$Q=\\begin{bmatrix}0.96276969 & -0.48963374\\\\0.27032301 & 0.87192821\\end{bmatrix}$$\n\n***&Lambda;*** is a matrix that contains the eigenvalues for ***A*** on the diagonal, with zeros in all other elements; so for a 2x2 matrix, &Lambda; will look like this:\n\n$$\\Lambda=\\begin{bmatrix}\\lambda_{1} & 0\\\\0 & \\lambda_{2}\\end{bmatrix}$$\n\nIn our Python code, we've already used the ***linalg.eig*** function to return the array of eigenvalues for ***A*** into the variable ***l***, so now we just need to format that as a matrix:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "L = np.diag(l)\nprint (L)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So ***&Lambda;*** is the following matrix:\n\n$$\\Lambda=\\begin{bmatrix}3.56155281 & 0\\\\0 & -0.56155281\\end{bmatrix}$$\n\nNow we just need to find ***Q<sup>-1</sup>***, which is the inverse of ***Q***:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "Qinv = np.linalg.inv(Q)\nprint(Qinv)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "The inverse of ***Q*** then, is:\n\n$$Q^{-1}=\\begin{bmatrix}0.89720673 & 0.50382896\\\\-0.27816009 & 0.99068183\\end{bmatrix}$$\n\nSo what does that mean? Well, it means that we can decompose the transformation of *any* vector multiplied by matrix ***A*** into the separate operations ***Q&Lambda;Q<sup>-1</sup>***:\n\n$$A\\vec{v} = Q \\Lambda Q^{-1}\\vec{v}$$\n\nTo prove this, let's take vector ***v***:\n\n$$\\vec{v} = \\begin{bmatrix}1\\\\3\\end{bmatrix} $$\n\nOur matrix transformation using ***A*** is:\n\n$$\\begin{bmatrix}3 & 2\\\\1 & 0\\end{bmatrix} \\cdot \\begin{bmatrix}1\\\\3\\end{bmatrix} $$\n\nSo let's show the results of that using Python:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "v = np.array([1,3])\nt = A@v\n\nprint(t)\n\n# Plot v and t\nvecs = np.array([v,t])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['orange', 'b'], scale=20)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "And now, let's do the same thing using the ***Q&Lambda;Q<sup>-1</sup>*** sequence of operations:"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "import math\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nt = (Q@(L@(Qinv)))@v\n\n# Plot v and t\nvecs = np.array([v,t])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['orange', 'b'], scale=20)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So ***A*** and ***Q&Lambda;Q<sup>-1</sup>*** are equivalent.\n\nIf we view the intermediary stages of the decomposed transformation, you can see the transformation using ***A*** in the original base for ***v*** (orange to blue) and the transformation using ***&Lambda;*** in the change of basis decribed by ***Q*** (red to magenta):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nt1 = Qinv@v\nt2 = L@t1\nt3 = Q@t2\n\n# Plot the transformations\nvecs = np.array([v,t1, t2, t3])\norigin = [0], [0]\nplt.axis('equal')\nplt.grid()\nplt.ticklabel_format(style='sci', axis='both', scilimits=(0,0))\nplt.quiver(*origin, vecs[:,0], vecs[:,1], color=['orange', 'red', 'magenta', 'blue'], scale=20)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So from this visualization, it should be apparent that the transformation ***Av*** can be performed by changing the basis for ***v*** using ***Q*** (from orange to red in the above plot) applying the equivalent linear transformation in that base using ***&Lambda;*** (red to magenta), and switching back to the original base using ***Q<sup>-1</sup>*** (magenta to blue)."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Rank of a Matrix\n\nThe **rank** of a square matrix is the number of non-zero eigenvalues of the matrix. A **full rank** matrix has the same number of non-zero eigenvalues as the dimension of the matrix. A **rank-deficient** matrix has fewer non-zero eigenvalues as dimensions. The inverse of a rank deficient matrix is singular and so does not exist (this is why in a previous notebook we noted that some matrices have no inverse).\n\nConsider the following matrix ***A***:\n\n$$A=\\begin{bmatrix}1 & 2\\\\4 & 3\\end{bmatrix}$$\n\nLet's find its eigenvalues (***&Lambda;***):"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nA = np.array([[1,2],\n              [4,3]])\nl, Q = np.linalg.eig(A)\nL = np.diag(l)\nprint(L)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\\Lambda=\\begin{bmatrix}-1 & 0\\\\0 & 5\\end{bmatrix}$$\n\nThis matrix has full rank. The dimensions of the matrix is 2. There are two non-zero eigenvalues. \n\nNow consider this matrix:\n\n$$B=\\begin{bmatrix}3 & -3 & 6\\\\2 & -2 & 4\\\\1 & -1 & 2\\end{bmatrix}$$\n\nNote that the second and third columns are just scalar multiples of the first column.\n\nLet's examine it's eigenvalues:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "B = np.array([[3,-3,6],\n              [2,-2,4],\n              [1,-1,2]])\nlb, Qb = np.linalg.eig(B)\nLb = np.diag(lb)\nprint(Lb)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "$$\\Lambda=\\begin{bmatrix}3 & 0& 0\\\\0 & -6\\times10^{-17} & 0\\\\0 & 0 & 3.6\\times10^{-16}\\end{bmatrix}$$\n\nNote that matrix has only 1 non-zero eigenvalue. The other two eigenvalues are so extremely small as to be effectively zero. This is an example of a rank-deficient matrix; and as such, it has no inverse."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Inverse of a Square Full Rank Matrix\nYou can calculate the inverse of a square full rank matrix by using the following formula:\n\n$$A^{-1} = Q \\Lambda^{-1} Q^{-1}$$\n\nLet's apply this to matrix ***A***:\n\n$$A=\\begin{bmatrix}1 & 2\\\\4 & 3\\end{bmatrix}$$\n\nLet's find the matrices for ***Q***, ***&Lambda;<sup>-1</sup>***, and ***Q<sup>-1</sup>***:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nA = np.array([[1,2],\n              [4,3]])\n\nl, Q = np.linalg.eig(A)\nL = np.diag(l)\nprint(Q)\nLinv = np.linalg.inv(L)\nQinv = np.linalg.inv(Q)\nprint(Linv)\nprint(Qinv)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "So:\n\n$$A^{-1}=\\begin{bmatrix}-0.70710678 & -0.4472136\\\\0.70710678 & -0.89442719\\end{bmatrix}\\cdot\\begin{bmatrix}-1 & -0\\\\0 & 0.2\\end{bmatrix}\\cdot\\begin{bmatrix}-0.94280904 & 0.47140452\\\\-0.74535599 & -0.74535599\\end{bmatrix}$$\n\nLet's calculate that in Python:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "Ainv = (Q@(Linv@(Qinv)))\nprint(Ainv)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "That gives us the result:\n\n$$A^{-1}=\\begin{bmatrix}-0.6 & 0.4\\\\0.8 & -0.2\\end{bmatrix}$$\n\nWe can apply the ***np.linalg.inv*** function directly to ***A*** to verify this:"
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(np.linalg.inv(A))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python36",
      "display_name": "Python 3.6",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}